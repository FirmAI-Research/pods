{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Estimating Test Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we learned several scores (accuracy, precision, recall, F1) for evaluating classification models. We calculated these scores on the training data---that is, the same data that was used to evaluate the model. In Chapter 5, we saw that evaluating machine learning models on the training data was problematic because a machine learning model could achieve a good training score by _overfitting_ to the training data. We argued that the goal of a machine learning model should be to achieve a good score on test data. (Chapter 5.4) However, the labels are often not known on the test data. Nevertheless, we can use cross-validation on the training data to estimate the test scores. (Chapter 5.5) These so-called _validation scores_ can be used to select between models and tune hyperparameters. (Chapter 5.6)\n",
    "\n",
    "Although Chapter 5 was about regression models, the exact same program can be carried out for classification models. Instead of calculating the *training* accuracy, precision, etc., we estimate the *test* accuracy, precision, etc. using cross-validation. This section demonstrates how to carry out this program, but the concepts and even the code are essentially the same as in Chapter 5.\n",
    "\n",
    "First, we define a model that we want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('kneighborsclassifier',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=10, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "data_dir = \"http://dlsun.github.io/pods/data/\"\n",
    "df_breast = pd.read_csv(data_dir + \"breast-cancer.csv\")\n",
    "\n",
    "X_train = df_breast[[\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\",\n",
    "                     \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \"Bare Nuclei\",\n",
    "                     \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\"]]\n",
    "y_train = df_breast[\"Class\"]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=10)\n",
    ")\n",
    "\n",
    "pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate test scores using $k$-fold cross validation, we use the `cross_val_score` function in scikit-learn. For example, to calculate test accuracy, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89855072, 0.95652174, 0.95652174, 0.94117647, 0.98529412,\n",
       "       0.95588235, 0.97058824, 0.98529412, 0.98529412, 1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
    "                            cv=10, scoring=\"accuracy\")\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 10 accuracy scores, one for each of the $k=10$ folds. It is customary to average these accuracy scores to obtain one overall estimate of the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635123614663257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is still high, but lower than the 97.2% training accuracy that we calculated in the previous lesson. This makes sense because it is always harder for a model to predict on data it has not seen than on data it has seen. Recall that Wenger's neural network model that won the Google Science Fair had an accuracy of 97.4%. We have come close to achieving that using a simple $10$-nearest neighbors classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn can also calculate the precision and recall of a class $c$, but the labels need to be converted to a binary label that is $1$ (or `True`) if the observation is in class $c$ and $0$ (or `False`) otherwise. For example, to calculate the precision for benign tumors (class 0), we define the new label `is_benign`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779051122632646"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_benign = (y_train == 0)\n",
    "\n",
    "cross_val_score(pipeline, X_train, is_benign, \n",
    "                cv=10, scoring=\"precision\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the validation _recall_ for benign tumors, we just have to change the scoring method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754040404040405"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, X_train, is_benign, \n",
    "                cv=10, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the validation precision and recall for malignant tumors is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9590172532781228, 0.9373188405797102)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_malignant = (y_train == 1)\n",
    "\n",
    "precision = cross_val_score(pipeline, X_train, is_malignant, \n",
    "                            cv=10, scoring=\"precision\").mean()\n",
    "recall = cross_val_score(pipeline, X_train, is_malignant, \n",
    "                         cv=10, scoring=\"recall\").mean()\n",
    "\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another term for recall is _sensitivity_. Wenger's model was 99.1% sensitive to malignancy; our model is quite a bit worse, with a sensitivity of only 93.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Could we do better with a different value of $k$? We use cross-validation on a grid of $k$ values and pick the one that maximizes some score. Since the F1 score combines precision and recall, we use F1 as the score. There is a different F1 score for the benign masses and the malignant masses; `_macro` specifies that we use the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kneighborsclassifier__n_neighbors': 13}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid={\"kneighborsclassifier__n_neighbors\": range(1, 50)},\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this value of $k$ better? It certainly has a higher average F1 score. What about its precision and recall for malignant masses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9590172532781228, 0.9640265700483092, 0.9373188405797102, 0.953985507246377)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_precision = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X_train, is_malignant,\n",
    "    scoring=\"precision\",\n",
    "    cv=10).mean()\n",
    "\n",
    "new_recall = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X_train, is_malignant,\n",
    "    scoring=\"recall\",\n",
    "    cv=10).mean()\n",
    "\n",
    "precision, new_precision, recall, new_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the new model has a higher precision _and_ a higher recall for malignancy. This suggests that the new model is unambiguously better. (If only recall had been higher, then it could be argued that we were simply trading off precision for recall.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Exercises 1-2 ask you to use the Titanic data set (`https://dlsun.github.io/pods/data/titanic.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Train a 5-nearest neighbors model to predict whether or not a passenger on a Titanic survived, using their age, sex, and class as features. Calculate the accuracy, precision, and recall of this model (on the training data) for the survivors and the deceased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. You want to build a $k$-nearest neighbors model to predict whether or not a passenger on the Titanic survived, using their age, sex, and class. \n",
    "\n",
    "- What value of $k$ optimizes overall accuracy?\n",
    "- What value of $k$ optimizes the F1 score for the deceased?\n",
    "\n",
    "Does the same value of $k$ optimize accuracy and the F1 score?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
